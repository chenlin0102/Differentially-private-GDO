{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion-MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di1ta99iOtTX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n74lvt59OzKi"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/privacy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKvYCLSLO2Wy"
      },
      "source": [
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfcfo8hYO5yZ"
      },
      "source": [
        "tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUW_aF4jO8BJ"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from time import strftime, localtime\n",
        "\n",
        "import os\n",
        "from absl import app\n",
        "from tensorflow.python.platform import flags\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from privacy.analysis import privacy_ledger\n",
        "from privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
        "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from privacy.optimizers import dp_optimizer\n",
        "\n",
        "if LooseVersion(tf.__version__) < LooseVersion('2.0.0'):\n",
        "    GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
        "    AdamOptimizer = tf.train.AdamOptimizer\n",
        "    AdagradOptimizer = tf.train.AdagradOptimizer\n",
        "    MomentumOptimizer = tf.train.MomentumOptimizer\n",
        "else:\n",
        "    GradientDescentOptimizer = tf.optimizers.SGD  # pylint: disable=invalid-name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE2r8cfEO-1z"
      },
      "source": [
        "model_base_path = '/home/fanghb/dp-sgd/fork/privacy/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3dJ8_LPA2R"
      },
      "source": [
        "# TODO 模型和tensorboard存放的位置\n",
        "filewriter_path = model_base_path + 'dpsgd' + \"_\" + 'momentum' + \"/tensorboard\"\n",
        "checkpoint_path = model_base_path + 'dpsgd' + \"_\" + 'momentum' + \"/checkpoints\"\n",
        "\n",
        "# Create parent path if it doesn't exist\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "if not os.path.isdir(filewriter_path):\n",
        "    os.makedirs(filewriter_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEOp08gAPDZh"
      },
      "source": [
        "X = tf.placeholder(tf.float32, [600, 28, 28])\n",
        "Y = tf.placeholder(tf.int64, [600])\n",
        "\n",
        "def cnn_model_fn(features, labels, mode):\n",
        "    \"\"\"Model function for a CNN.\"\"\"\n",
        "\n",
        "    # Define CNN architecture using tf.keras.layers.\n",
        "    \n",
        "    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
        "    \n",
        "\n",
        "    y = tf.keras.layers.Conv2D(16, 8,\n",
        "                               strides=2,\n",
        "                               padding='same',\n",
        "                               activation='relu').apply(input_layer)\n",
        "    y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
        "    y = tf.keras.layers.Conv2D(32, 4,\n",
        "                               strides=2,\n",
        "                               padding='valid',\n",
        "                               activation='relu').apply(y)\n",
        "    y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
        "    y = tf.keras.layers.Flatten().apply(y)\n",
        "    y = tf.keras.layers.Dense(32, activation='relu').apply(y)\n",
        "    logits = tf.keras.layers.Dense(10).apply(y)\n",
        "\n",
        "    # Calculate accuracy.\n",
        "    correct_pred = tf.equal(tf.argmax(logits, 1), labels)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
        "    vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=labels, logits=logits)\n",
        "    # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
        "    scalar_loss = tf.reduce_mean(vector_loss)\n",
        "\n",
        "    \n",
        "    ledger = privacy_ledger.PrivacyLedger(\n",
        "            population_size=60000,\n",
        "            selection_probability=(600 / 60000))\n",
        "\n",
        "        # Use DP version of GradientDescentOptimizer. Other optimizers are\n",
        "        # available in dp_optimizer. Most optimizers inheriting from\n",
        "        # tf.train.Optimizer should be wrappable in differentially private\n",
        "        # counterparts by calling dp_optimizer.optimizer_from_args().\n",
        "        \n",
        "       \n",
        "    optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
        "                l2_norm_clip=4.0,\n",
        "                noise_multiplier=4.0,\n",
        "                num_microbatches=300,\n",
        "                ledger=ledger,\n",
        "                learning_rate=0.001,\n",
        "                )\n",
        "\n",
        "        \n",
        "    opt_loss = vector_loss\n",
        "    \n",
        "    \n",
        "    global_step = tf.train.get_global_step()\n",
        "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
        "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
        "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
        "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
        "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
        "    return train_op, scalar_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk47MarIPHhx"
      },
      "source": [
        "def load_Fashionmnist():\n",
        "    \"\"\"Loads Fashion-MNIST and preprocesses to combine training and validation data.\"\"\"\n",
        "    train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    train_data, train_labels = train\n",
        "    test_data, test_labels = test\n",
        "\n",
        "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "    train_labels = np.array(train_labels, dtype=np.int32)\n",
        "    test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "    print(train_labels.shape, test_data.shape)\n",
        "\n",
        "    assert train_data.min() == 0.\n",
        "    assert train_data.max() == 1.\n",
        "    assert test_data.min() == 0.\n",
        "    assert test_data.max() == 1.\n",
        "    assert train_labels.ndim == 1\n",
        "    assert test_labels.ndim == 1\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYrz8ONQPOAx"
      },
      "source": [
        "def generate_next_batch(data, label, batch_size, shffule=False):\n",
        "    print(type(data), data.shape, label.shape, batch_size, \"---\", data.shape[0], data.shape[1], data.shape[2])\n",
        "    if shffule:\n",
        "        permutation = np.random.permutation(data.shape[0])\n",
        "        data = data[permutation]\n",
        "        label = label[permutation]\n",
        "    size = data.shape[0] // batch_size\n",
        "    print(batch_size, \"()()\", size, \"()\", data.shape[0])\n",
        "    for i in range(size):\n",
        "        yield (data[i * batch_size: (i + 1) * batch_size], label[i * batch_size: (i + 1) * batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbPWPhRGPP8R"
      },
      "source": [
        "train_op, opt_loss, opt_accuracy = cnn_model_fn(X, Y,None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtUgxYZyPQZJ"
      },
      "source": [
        "tf.summary.scalar('loss', opt_loss)\n",
        "tf.summary.scalar('accuracy', opt_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOBCH2g-PSR7"
      },
      "source": [
        "train_data, train_labels, test_data, test_labels = load_Fashionmnist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrG_zuZiPUQB"
      },
      "source": [
        "def main(unused_argv):\n",
        "    merged = tf.summary.merge_all()\n",
        "    # Initialize the FileWriter\n",
        "    writer = tf.summary.FileWriter(filewriter_path)\n",
        "\n",
        "    # Initialize an saver for store model checkpoints\n",
        "    saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
        "\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        tf.logging.set_verbosity(tf.logging.INFO)\n",
        "        \n",
        "\n",
        "        # Load training and test data.\n",
        "        print(train_data.shape, train_labels.shape)\n",
        "        # Training loop.\n",
        "        for epoch in range(1, 100 + 1):\n",
        "\n",
        "            print(\"At epcho %d\" % epoch)\n",
        "\n",
        "            # Train the model for one epoch.\n",
        "            gen = generate_next_batch(data=train_data, label=train_labels, batch_size=600, shffule=True)\n",
        "            train_acc = 0.\n",
        "            train_loss = 0.\n",
        "            train_count = 0\n",
        "            for img_batch, label_batch in gen:\n",
        "                _, loss, acc = sess.run([train_op, opt_loss, opt_accuracy], feed_dict={X: img_batch,\n",
        "                                                                                       Y: label_batch})\n",
        "                train_acc += acc\n",
        "                train_loss += loss\n",
        "                train_count += 1\n",
        "            train_acc /= train_count\n",
        "            train_loss /= train_count\n",
        "            print('Train accuracy: {}, Train loss : {},  after {} epochs'.format(train_acc, train_loss, epoch))\n",
        "\n",
        "            gen = generate_next_batch(data=test_data, label=test_labels, batch_size=600, shffule=False)\n",
        "            test_acc = 0.\n",
        "            test_loss = 0.\n",
        "            test_count = 0\n",
        "            for img_batch, label_batch in gen:\n",
        "                loss, acc = sess.run([opt_loss, opt_accuracy], feed_dict={X: img_batch,\n",
        "                                                                          Y: label_batch})\n",
        "                test_acc += acc\n",
        "                test_loss += loss\n",
        "                test_count += 1\n",
        "            test_acc /= test_count\n",
        "            test_loss /= test_count\n",
        "            print('Test accuracy: {}, Test loss : {},  after {} epochs'.format(test_acc, test_loss, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_goIjaxPZOp"
      },
      "source": [
        "tf.app.run(main)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}